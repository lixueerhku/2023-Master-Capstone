{"0": {"text": " parallel algorithms, very large scale integration, rectangle intersection problem, plane sweep problem, computational geometry, design rule checking, parasitic resistance capacitance extraction, mask processing flows, multicore machines, clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " plane sweep, multicore, VLSI, design rule, mask processing.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " VLSI physical design, computational geometry, design rule checking, RCP extraction, mask processing, embarrassingly parallel, multicore machines, clusters, best known algorithms, real world tests.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, Very Large Scale Integration, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Parasitic Resistance Capacitance Extraction, Mask Processing Flows, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, VLSI Physical Design, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Parasitic Resistance Capacitance Extraction, Mask Processing Flows, Embarrassingly Parallel, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, VLSI Physical Design, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Resistance Capacitance Extraction, Mask Processing Flows, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " information technology adoption, perceptions, independent variables, antecedents, consequents, expert systems application, technology acceptance models, personal innovativeness, mass media, interpersonal communication", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " user perceptions, information technology, adoption decisions, personal innovativeness, mass media, interpersonal communication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Expert Systems Application, Technology Acceptance Models, Personal Innovativeness, Mass Media, Interpersonal Communication, Perception Development, Adoption Decisions", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nInformation Technology Adoption, Perceptions, Independent Variables, Expert Systems Application, Moderating Influences, Personal Innovativeness, Mass Media, Interpersonal Communication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " information technology adoption, perceptions, independent variables, antecedents, consequents, expert systems application, technology acceptance models, personal innovativeness, mass media, interpersonal communication, perception development, theoretical implications, practical implications.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Expert Systems Application, Technology Acceptance Models, Personal Innovativeness, Mass Media, Interpersonal Communication, Perception Development, Adoption Decisions, Information Technology Adoption", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, discrete latent state space, principal component analysis, handwritten digit recognition, satellite image recognition", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variables, generative models, classification accuracy, support vector, perceptron learning, discrete latent, principal component, handwritten digit, satellite image.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, deterministic models, support vector machine, perceptron learning, discrete latent state, principal component analysis, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, discrete latent state space, continuous latent state space, support vector machine, perceptron learning, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, perceptron learning, discrete latent state, continuous latent state, handwritten digit recognition, satellite image recognition, sequential constraint optimization.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, discrete latent state, principal component analysis, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction Methods, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction, Event Related Potentials, EEG Data, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time, Feature Extraction, Classifiers, Event Related Potentials, EEG Data, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time, Event Related Potentials, Feature Extraction, Classification, Accuracy, Processing Time, EEG Data, Hex O Speller, Prototype System.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time Implementation, Feature Extraction Methods, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, numerical examples, ifr property, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " e m e k, ifr property, interdeparture time, erlang distributions, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interarrival time, service time, erlang distributions, numerical examples, interdeparture time, e m e k [digit] queues, ifr property, theoretical analysis.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nIncreasing Failure Rate, Interdeparture Time, Erlang Distributions, EMEK Queues, Numerical Examples", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, theoretical analysis, numerical results, numerical examples, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, numerical examples, ifr property, theoretical analysis.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " low complexity, quadraphonic headsets, surround audio, head related transfer function, simplified reverberator, sound localization, acoustical pole and zero, subjective listening tests, stereo headphones, privacy, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " surround audio, hrtf, capz model, stereo headphones, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " 5.1 channel hearing experience, sound localization capability, acoustical pole and zero model, subjective listening tests, four channel headset, stereo headphones, low cost privacy.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quadraphonic Headsets, Surround Audio, Head Related Transfer Function (HRTF), Moorer's Reverberator, Common Acoustical Pole and Zero (CAPZ) Model", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quadraphonic Headsets, Surround Audio, Head Related Transfer Function, HRTF Model, Simplified Reverberator, Sound Localization, CAPZ Model, Power Consumption, Stereo Headphones, Off-the-Shelf Components.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " four channel headset, 5.1 channel hearing experience, head related transfer function, simplified reverberator, sound localization capability, acoustical pole and zero model, subjective listening tests, stereo headphones, privacy, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, service security, security policy, security personalization, security policy personalization, security personalization approach, personalized security policy.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, security policy, security personalization, service customer, service provider, security preferences.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " malicious individuals, security requirements, security policy, security personalization, flexible security, security policy personalization, security personalization prototype", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nInternet services, E-commerce, E-health, Security policy, Security personalization, Security measures, Service customer, Service provider, Security preferences, Security policy personalization, Security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e-commerce, e-health, service security, security policy, security personalization, security preferences, service providers, service customers, security policy personalization, security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, service security, security policy, security personalization, security preferences, service providers, service customers, personalized security policy, security policy personalization, security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, networks on chip, GALS architectures, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network, portable alliance CMOS, standard cell library, [digit] nm CMOS fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " networks on chip, asynchronous converters, synchronous communication, cmos standard, cmos fabrication, spice simulation", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network on chip, various trade offs, latency and robustness, portable alliance cmos, spice simulation, [digit] nm cmos fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nSynchronous Communication, Asynchronous Communication, Networks on Chip, GALS Architectures, CMOS Standard Cell Library", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, networks on chip, GALS architectures, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network, portable alliance CMOS, standard cell library, [digit] nm CMOS fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network on chip, latency and robustness, portable alliance cmos, spice simulation, [digit] nm cmos fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling and characterization, organization modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, organizational key data, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " process oriented, lead time, degree of autonomy, improvement potentialities, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organization modeling system, structuring strategies, dynamical analysis, evaluation module, logistical key data, organizational key data, improvement potentialities, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " Simulation Aided Design, Manufacturing Systems, Structuring Strategies, Forecasted Order Program, Activity Networks, Organization Modeling System, Process Oriented Strategy, Dynamical Analysis, Simulation Tool Femos, Logistical Key Data, Organizational Key Data, Gear Boxes, Robot Grip Arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling and characterization, organization modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, degree of autonomy, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, organizational key data, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " selectors, combinatorial structures, pseudo random generators, broadcasting protocols, wake up problem, coin weighting problems, non constructive bounds, common technique.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " selectors, combinatorial objects, pseudo random, broadcasting, wake up, test sets, coin weighting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " explicit constructions, combinatorial objects, pseudo random generators, deterministic protocols, broadcasting, unknown directed radio networks, wake up problem, coin weighting problems, non constructive bounds, common technique.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nSelectors, Combinatorial Structures, Pseudo Random Generators, Broadcasting Protocols, Coin Weighting Problems", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " explicit constructions, combinatorial objects, selectors, selective families, pseudo random generators, broadcasting protocols, wake up problem, coin weighting problems, deterministic protocols, unknown directed radio networks.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " broadcasting protocols, coin weighting problems, directed radio networks, deterministic protocols, pseudo random generators, selective families, test sets, wake up problem", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, membrane analogy, maximum shearing stress, prismatic torsional members, fortran code, delphi based environment, multiple level application, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " h refinement, membrane analogy, maximum shearing stress, torsional analysis, l section.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " membrane analogy, h refinement type, a posteriori error estimates, maximum shearing stress, prismatic torsional members, integrated fortran code, delphi based environment, multiple level application, analytic solutions, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nMembrane Analogy, Finite Element Refinement, Torsional Problems, Maximum Shearing Stress, A Posteriori Error Estimates", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, a posteriori error estimates, graphical procedure, maximum shearing stress, prismatic torsional members, membrane analogy, multiple level application, analytic solutions, l section member, very accurate calculation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, a posteriori error estimates, maximum shearing stress, membrane analogy, fortran code, delphi based environment, multiple level application, cross sectional geometries, analytic solutions, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " RNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Parallel Processing, Duplicate Processing, Modular Multiplication", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " rns montgomery, modular exponentiation, base transformations, duplicate processing, parallel processing, modular multiplication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " modular exponentiation, residue number systems, parallel processing, base transformations, montgomery multiplication, speedup, duplicate processing, upper limit, modular multiplication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nRNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Parallel Processing", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " RNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Duplicate Processing, Parallel Processing, Speedup, Modular Multiplication, Upper Limit, Number of Parallel Processing Units.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " modular exponentiation, residue number systems, rns montgomery multiplication, base transformations, parallel processing, speedup, modular multiplication, upper limit.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, Grounded Design, Usability Problem, Value, HCI, Contextual Description, Prescriptive Approaches", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " quality in use, grounded design, usability problem, value, hci.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " grounded design, usability problem, contextual description, prescriptive approaches, value in the world, potential of HCI", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, HCI, Grounded Design, Usability Problem, Value in the World.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, HCI, Grounded Design, Usability Problem, Contextual Description, Value in the World, Fit between Designs, Expected Contexts of Use, Prescriptive Approaches, Dilemmas about Value.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, Grounded Design, Usability Problem, Value Delivery, Contextual Description, Prescriptive Approaches.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, vertebrate family [digit] GPCR members, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit], gpcrs, fugu rubripes, vertebrates, phylogenetic analysis, calr cgrpr, crf.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Japanese pufferfish, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu Rubripes, vertebrates, phylogenetic analysis, Calr CGRPR, CRF.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu Rubripes, vertebrates, phylogenetic analysis, Calr Cgrpr, CRF, receptor group, common ancestral gene, Japanese Pufferfish, comparative study.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, vertebrate family [digit] GPCR members, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " blotto game, low complexity, fair multiuser, subcarrier allocation, uplink OFDMA networks, limited budget, optimal throughput, competitive fairness, Nash equilibrium, low complexity algorithm, uncorrelated fading channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " blotto game, ofdma networks, limited budget, nash equilibrium, low complexity", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA, Power Allocation, Budget Allocation, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading, Correlated Fading.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA Networks", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA Networks, Limited Budget, Optimal Throughput, Competitive Fairness, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading Channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Subcarrier Allocation, OFDMA Networks, Limited Budget, Power Allocation, Budget Allocation, Competitive Fairness, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading Channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, glass transition temperature, relative dielectric constant, loss factors, curing agent content, mekp concentrations, styrene content, average molecular mass, vickers hardness, tape casting, relative dielectric constant.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, mekp amount, styrene content, relative dielectric constant, loss factor, molecular mass, viscosity, tape casting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, glass transition temperature, relative dielectric constant, loss factors, curing agent content, mekp concentrations, styrene content, average molecular mass, vickers hardness, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, curing agent, styrene content, glass transition temperature, relative dielectric constant, loss factors, molecular mass, vickers hardness, mekp concentrations, film formation, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, unsaturated polyesterstyrene copolymers, glass transition temperature, relative dielectric constant, loss factors, curing agent, mekp amount, styrene content, average molecular mass, vickers hardness, tape casting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, curing agent, styrene content, glass transition temperature, relative dielectric constant, loss factors, molecular mass, vickers hardness, mekp concentrations, film formation, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " online information, pseudo online information, stroke order independent information, classification decisions, cursive word recognition, single engine, pure online information", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " online data, pseudo online, cursive word, single engine.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, single engine, classification decisions, online cursive words, offline representation, improved recognition", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nOnline Information, Pseudo Online Information, Stroke Order Independent Information, Cursive Word Recognition, Classification Decisions, Single Engine, Pure Online Information.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, online cursive words, single engine, classification decisions, offline representation, online signal, combination of classifiers, novel method, improved recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, online cursive words, single engine, classification decisions, offline representation, online signal, word recognition, combination of classifiers.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " gaussian pulses, mobile telephony, gmsk signals, uwb communications, third order derivatives, rectangular pulses, spectral mask, relative error, energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " gaussian pulses, gsm, uwb, derivatives, rectangular pulses, filter.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " mobile telephony (GSM), UWB communications, second order derivatives, third order derivatives, spectral mask requirements, relative error, energy spectra", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, Communications, Mobile telephony, GMSK signals, UWB communications, Derivatives, Rectangular pulses, Filter, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, GSM signals, UWB communications, Physical implementations, Third order derivatives, Rectangular pulses, Fixed amplitudes, Standard techniques, Spectral mask requirements, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, Mobile telephony, GMSK signals, UWB communications, Third order derivatives, Rectangular pulses, Spectral mask, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, convex semi definite programming, support vector machines, expected risk, margin, support vectors, data compression, generalization bound, prediction ability, robust form, soft idea, numerical results, generalization ability, regular pca.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, svpca, svms, convexity, semi definite", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, support vector machines, expected risk, margin, semi definite programming, data compression, generalization bound, prediction ability, robust form, convexity, numerical results, generalization ability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Linear PCA, Support Vector Machines, Expected Risk, Margin, Semi Definite Programming, Weighted Case, Generalization Bound, Soft Idea, Convexity, Local Minima, Numerical Results, Generalization Ability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Linear PCA, Convex Semi Definite Programming, Support Vector Machines, Expected Risk, Margin, Support Vectors, Data Compression, Generalization Bound, Soft Idea, Robust Form, Local Minima, Numerical Results.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, convex semi definite programming, support vector machines, expected risk, margin, support vectors, data compression, generalization bound, prediction ability, robust form, soft idea, numerical results, generalization ability, local minima.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, NAT, Metathesaurus, Semantic Network, Semantic Type, Diagrammatic Displays, Hybrid Diagram Text Interface, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " umls, nat, auditing, hybrid interface, semantic type, metathesaurus, concept networks.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS auditing, Neighborhood Auditing Tool, Hybrid Diagram Text Interface, Metathesaurus Level, Semantic Network Level, Case Studies, Select Group of Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Source Vocabularies, Metathesaurus, Semantic Network, Diagrammatic Displays, Text Windows, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Source Vocabularies, Inconsistencies, Focus Concept, Concept Networks, Diagrammatic Displays, Metathesaurus, Semantic Network, Semantic Type, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Semantic Network, Metathesaurus, Concept Network, Diagrammatic Displays, Text Windows, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
