{"0": {"text": " feedback vertex set, subgraph, [digit] degenerate, cardinality, minimum feedback vertex set, vertex, dg (v), polynomial time algorithm, 2n [digit], n [digit].", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " feedback vertex set, [digit] degenerate, forest, subgraph, vertex, algorithm.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " feedback vertex set, subgraph induced, minimum feedback vertex set, [digit] degenerate, vertex graph, polynomial time algorithm, cardinality at most", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " Feedback Vertex Set, Graph, Subgraph, Degenerate Graphs, Cardinality, Minimum Feedback Vertex Set, Vertex, Algorithm.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " feedback vertex set, subgraph, forest, minimum feedback vertex set, [digit] degenerate graphs, vertex, dg (v), 2n [digit], polynomial time algorithm, cardinality, tight bound.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " feedback vertex set, subgraph, [digit] degenerate, cardinality, minimum feedback vertex set, vertex, polynomial time algorithm, forest, tight bound, consequence.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a feedback vertex set of [digit] degenerate graphs .\nDocument Body: a feedback vertex set of a graph g is a set s of its vertices such that the subgraph induced by v ( g ) s v ( g ) s is a forest . the cardinality of a minimum feedback vertex set of g is denoted by ( g ) ( g ) . a graph g is [digit] degenerate if each subgraph g g of g has a vertex v such that dg ( v ) [digit] d g ( v ) [digit] . in this paper , we prove that ( g ) 2n [digit] ( g ) [digit] n [digit] for any [digit] degenerate n vertex graph g and moreover , we show that this bound is tight . as a consequence , we derive a polynomial time algorithm , which for a given [digit] degenerate n vertex graph returns its feedback vertex set of cardinality at most 2n [digit] [digit] n [digit] .\nKeyphrases (separated by comma):"}
{"0": {"text": " hybrid analytical modeling, pending cache hits, data prefetching, mshrs, performance impact, hardware prefetching, miss status holding register resources, superscalar microprocessors, timeliness of pending hits, prefetches, limited number of mshrs, non uniform dram memory latency, moving average of memory access latency.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": " pending cache hits, data prefetching, mshrs, hybrid analytical models, timeliness, limited mshrs, non uniform dram.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": " hybrid analytical modeling, performance impact, pending cache hits, hardware prefetching, miss status holding register, timeliness of pending hits, limited number of mshrs, non uniform dram memory latency, moving average of memory access latency.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nPending Cache Hits, Data Prefetching, MSHRs, Hybrid Analytical Models, Superscalar Microprocessors", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": " hybrid analytical modeling, pending cache hits, data prefetching, mshrs, performance impact, hardware prefetching, miss status holding register resources, superscalar microprocessors, timeliness of pending hits, limited number of mshrs, non uniform dram memory latency, moving average of memory access latency.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": " hybrid analytical modeling, pending cache hits, data prefetching, miss status holding register resources, superscalar microprocessors, timeliness of pending hits, limited number of mshrs, non uniform dram memory latency, moving average of memory access latency.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: hybrid analytical modeling of pending cache hits , data prefetching , and mshrs .\nDocument Body: this article proposes techniques to predict the performance impact of pending cache hits , hardware prefetching , and miss status holding register resources on superscalar microprocessors using hybrid analytical models . the proposed models focus on timeliness of pending hits and prefetches and account for a limited number of mshrs . they improve modeling accuracy of pending hits by 3.9 x and when modeling data prefetching , a limited number of mshrs , or both , these techniques result in average errors of 9.5 % to 17.8 % . the impact of non uniform dram memory latency is shown to be approximated well by using a moving average of memory access latency .\nKeyphrases (separated by comma):"}
{"0": {"text": " autoimmune polyendocrinopathy candidiasis ectodermal dystrophy, autoimmune regulator gene, chronic mucocutaneous candidiasis, autoimmunity, tolerance, pathogenesis", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " apeced, aire, autoimmunity, tolerance, pathogenesis.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " monogenic autosomal recessive disease, chronic mucocutaneous candidiasis, autoimmune diseases, autoimmune regulator (aire) gene, tolerance and autoimmunity, pathogenesis of the disease.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " Autoimmune Polyendocrinopathy Candidiasis Ectodermal Dystrophy (APECED), Autoimmune Regulator (AIRE) gene, Chronic Mucocutaneous Candidiasis, Autoimmune Diseases, Tolerance and Autoimmunity, Pathogenesis of the Disease.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " Autoimmune Polyendocrinopathy Candidiasis Ectodermal Dystrophy, Aire Gene, Chronic Mucocutaneous Candidiasis, Autoimmune Diseases, Immunological Features, Pathogenesis, Tolerance, Autoimmunity, Alternative Models, Clinical Presentation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " autoimmune polyendocrinopathy candidiasis ectodermal dystrophy, autoimmune regulator gene, chronic mucocutaneous candidiasis, autoimmunity, tolerance, pathogenesis, alternative models.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy known and novel aspects of the syndrome .\nDocument Body: autoimmune polyendocrinopathy candidiasis ectodermal dystrophy ( apeced ) is a monogenic autosomal recessive disease caused by mutations in the autoimmune regulator ( aire ) gene and , as a syndrome , is characterized by chronic mucocutaneous candidiasis and the presentation of various autoimmune diseases . during the last decade , research on apeced and aire has provided immunologists with several invaluable lessons regarding tolerance and autoimmunity . this review describes the clinical and immunological features of apeced and discusses emerging alternative models to explain the pathogenesis of the disease .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, two phase stefan problem, three dimensional domain, convective heat transfer, characteristics method, finite element method, numerical algorithm, numerical results, industrial experimental measurements.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, two phase, three dimensional, numerical solution, convective heat, finite element, nonlinear discretized.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, two phase stefan problem, convective heat transfer, characteristics method, finite element method, nonlinear discretized problem, numerical algorithm, industrial experimental measurements.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, two phase stefan problem, three dimensional domain, convective heat transfer, numerical algorithm, numerical results, industrial experimental measurements.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, three dimensional solidification, enthalpy formulation, two phase stefan problem, convective heat transfer, characteristics method, finite element method, nonlinear discretized problem, numerical algorithm, industrial experimental measurements.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " aluminium casting, two phase stefan problem, convective heat transfer, characteristics method, finite element method, nonlinear discretized problem, numerical algorithm, numerical solution, industrial experimental measurements.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: numerical solution of a three dimensional solidification problem in aluminium casting .\nDocument Body: in this paper , we consider an enthalpy formulation for a two phase stefan problem arising from the solidification of aluminium during casting process . we solve this free boundary problem in a time varying three dimensional domain and consider convective heat transfer in the liquid phase . the resulting equations are discretized using a characteristics method in time and a finite element method in space , and we propose a numerical algorithm to solve the obtained nonlinear discretized problem . finally , numerical results are given which are compared with industrial experimental measurements .\nKeyphrases (separated by comma):"}
{"0": {"text": " rib features, aircraft structural parts, object oriented software engineering, constrained faces, machining operations, CNC operation, machining tool paths, feature recognition algorithm, attribute adjacency graph, local rib elements, cluster groups, optimised cluster groups, enhanced recognition algorithm, aircraft manufacturer.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " rib features, aircraft structural parts, cnc, machining, feature recognition, attribute adjacency, local rib elements, rib features.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " aircraft structural parts, rib machining operations, computerized numerical control, feature recognition algorithm, attribute adjacency graph, local rib elements, optimised cluster groups, top level rib features, advanced aircraft structural parts, large aircraft manufacturer.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " Rib Features, Aircraft Structural Parts, Object Oriented Software Engineering, CNC Operation, Machining Knowledge, Process Planning, Feature Recognition Algorithm, Attribute Adjacency Graph, Local Rib Elements, Cluster Groups, Optimised Cluster Groups, Enhanced Recognition Algorithm, Pilot System, Advanced Aircraft Structural Parts, Large Aircraft Manufacturer.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " rib features, aircraft structural parts, object oriented software engineering, constrained faces, machining operations, CNC operation, machining tool paths, feature recognition algorithm, attribute adjacency graph, local rib elements, cluster groups, optimised cluster groups, enhanced recognition algorithm, advanced aircraft structural parts.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " rib features, aircraft structural parts, object oriented software engineering, constrained faces, machining operations, CNC operation, machining tool paths, feature recognition algorithm, attribute adjacency graph, local rib elements, cluster groups, optimised cluster groups, enhanced recognition algorithm, advanced aircraft structural parts.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: definition and recognition of rib features in aircraft structural part .\nDocument Body: in this research , a new type of manufacturing feature that is commonly observed in aircraft structural parts , known as ribs , is defined and implemented using the object oriented software engineering approach . the rib feature type is defined as a set of constrained and adjacent faces of a part which are associated with a set of specific rib machining operations . computerized numerical control ( cnc ) operation experience and the machining knowledge are leveraged by analysing typical geometry interactions when generating machining tool paths where such knowledge and experience are abstracted as rules of process planning . then those abstracted machining process rules are implemented in a feature recognition algorithm on top of an existing and holistic attribute adjacency graph solution to extract seed faces , identify individual local rib elements and further cluster these newly identified local rib elements into groups for the ease of machining operations . out of the potentially different combinations of local rib elements , those optimised cluster groups are merged into the top level rib features . the enhanced recognition algorithm is presented in details . a pilot system has already been developed and applied for machining many advanced aircraft structural parts in a large aircraft manufacturer . observations and conclusions are presented at the end .\nKeyphrases (separated by comma):"}
{"0": {"text": " harmonic balance method, nonlinear networks, grobner base, algebraic representation, error bound, computational cost, singular points, break points.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " hb method, grobner base, error bound, singular points.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " algebraic representation, grobner base, harmonic balance method, error bound, singular points, break points, computational cost, truncation error.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " Harmonic Balance Method, Nonlinear Networks, Systems, Truncation Error, Grobner Base, Algebraic Representation, Error Bound, Computational Cost, Singular Points, Break Points.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " harmonic balance method, nonlinear networks, grobner base, algebraic representation, error bound, computational cost, singular points, break points, numerical computation, truncation error.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " harmonic balance method, nonlinear networks, grobner base, algebraic representation, error bound, computational cost, singular points, break points.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an algebraic approach to guarantee harmonic balance method using grobner base .\nDocument Body: harmonic balance ( hb ) method is well known principle for analyzing periodic oscillations on nonlinear networks and systems . because the hb method has a truncation error , approximated solutions have been guaranteed by error bounds . however , its numerical computation is very time consuming compared with solving the hb equation . this paper proposes an algebraic representation of the error bound using grobner base . the algebraic representation enables to decrease the computational cost of the error bound considerably . moreover , using singular points of the algebraic representation , we can obtain accurate break points of the error bound by collisions .\nKeyphrases (separated by comma):"}
{"0": {"text": " wireless sensor networks, service oriented requirement, graph coloring, time division multiple access scheduling, distributed vertex coloring algorithm, intra cluster delay, intra cluster throughput, real time performance, communication reliability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " graph coloring, tdma scheduling, wireless sensor networks, service oriented, distributed graph, distance [digit], independent set, intra cluster, real time, communication reliability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " wireless sensor networks, service oriented requirement, distributed graph coloring, time division multiple access scheduling, distance [digit] coloring algorithm, independent set, network structure, intra cluster delay, intra cluster throughput, real time performance, communication reliability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " Wireless Sensor Networks, Service Oriented Requirement, Graph Coloring, Time Division Multiple Access, Clustering Based Sensor Network", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " Wireless Sensor Networks, Service Oriented Requirement, Graph Coloring, Time Division Multiple Access, Clustering, Distance [Digit] Coloring, Intra Cluster Delay, Intra Cluster Throughput, Real Time Performance, Communication Reliability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " Wireless Sensor Networks, Service Oriented Requirement, Graph Coloring, Time Division Multiple Access, Clustering, Distance [digit] Coloring, Intra Cluster Delay, Intra Cluster Throughput, Real Time Performance, Communication Reliability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a graph coloring based tdma scheduling algorithm for wireless sensor networks .\nDocument Body: wireless sensor networks should provide with valuable service , which is called service oriented requirement . to meet this need , a novel distributed graph coloring based time division multiple access scheduling algorithm ( gcsa ) , considering real time performance for clustering based sensor network , is proposed in this paper , to determine the smallest length of conflict free assignment of timeslots for intra cluster transmissions . gcsa involves two phases . in coloring phase , networks are modeled using graph theory , and a distributed vertex coloring algorithm , which is a distance [digit] coloring algorithm and can get colors near to ( ( updelta [digit] ) ) , is proposed to assign a color to each node in the network . then , in scheduling phase , each independent set is mapped to a unique timeslot according to the sets priority which is obtained by considering network structure . the experimental results indicate that gcsa can significantly decrease intra cluster delay and increase intra cluster throughput , which satisfies real time performance as well as communication reliability .\nKeyphrases (separated by comma):"}
{"0": {"text": " model as a service, maas, geoscience modeling, web interface, configuring machines, running models, managing model outputs, geoscientists, computing demands.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " maas, geoscience, models, web interface.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " geoscience modeling, model as a service, maas, web interface, configuring machines, setting up models, managing model outputs, computing demands", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " model as a service, geoscience modeling, web interface, configuring machines, setting up and running models, managing model outputs, geoscientists, computing demands.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " model as a service, geoscience modeling, web interface, configuring machines, setting up models, running models, managing model outputs, computing demands, geoscience models, geoscientists solutions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " model as a service, geoscience modeling, web interface, configuring machines, setting up models, managing model outputs, computing demands, geoscientists solutions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: building model as a service to support geosciences .\nDocument Body: model as a service ( maas ) concept and architecture is introduced to support geoscience modeling . maas enables various geoscience models to be published as services that can be accessed through a simple web interface . maas automates the processes of configuring machines , setting up and running models , and managing model outputs . maas provides new guidance for geoscientists seeking solutions to address the computing demands for geoscience models .\nKeyphrases (separated by comma):"}
{"0": {"text": " shot change detection, camera motion, brightness constraint, optical flow analysis, scene change, robustness, experimental results, precision performance.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": " shot change, camera motion, optical flow, brightness constraint, scene change.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": " shot change detection, camera motion, optical flow analysis, total brightness, scene point, smooth camera motion, qualitative aspect, significant know how, experimental results, precision performance.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nShot Change Detection, Scene Based Constraint, Video Database, Camera Motion, Optical Flow Analysis, Brightness Constraint, Scene Change, Qualitative Aspect, Robustness, Experimental Results, Precision Performance.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": " shot change detection, scene based constraint, video database, camera motion, optical flow analysis, total brightness, scene point, smooth camera motion, qualitative aspect, robustness, experimental results, precision performance.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": " shot change detection, camera motion, optical flow analysis, total brightness, scene point, smooth camera motion, qualitative aspect, significant know how, experimental results, precision performance.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: shot change detection using scene based constraint .\nDocument Body: a key step for managing a large video database is to partition the video sequences into shots . past approaches to this problem tend to confuse gradual shot changes with changes caused by smooth camera motions . this is in part due to the fact that camera motion has not been dealt with in a more fundamental way . we propose an approach that is based on a physical constraint used in optical flow analysis , namely , the total brightness of a scene point across two frames should remain constant if the change across two frames is a result of smooth camera motion . since the brightness constraint would be violated across a shot change , the detection can be based on detecting the violation of this constraint . it is robust because it uses only the qualitative aspect of the brightness constraint detecting a scene change rather than estimating the scene itself . moreover , by tapping on the significant know how in using this constraint , the algorithm 's robustness is further enhanced . experimental results are presented to demonstrate the performance of various algorithms . it was shown that our algorithm is less likely to interpret gradual camera motions as shot changes , resulting in a significantly better precision performance than most other algorithms .\nKeyphrases (separated by comma):"}
{"0": {"text": " tail asymptotics, hol priority queues, large number of independent stationary sources, buffer occupancy distribution, strict hol priority rule, long range dependent sources, two buffers, asymptotic upper bound, scaling parameter, server speed, buffer level, source pooling, numerical validation, reduced service rate approximation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " tail asymptotics, hol priority, large number, independent sources, two buffers, m buffers, source pooling.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " tail asymptotics, hol priority queues, large number of sources, long range dependent sources, strict priority rule, two buffers, m buffers, source pooling, numerical validation, reduced service rate approximation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " Tail Asymptotics, HOL Priority Queues, Large Number of Sources, Two Buffers, Strict Priority, Lower Priority Buffer, Scaling Parameter, Server Speed, Buffer Level, Source Pooling, Numerical Validation, Reduced Service Rate Approximation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " tail asymptotics, hol priority queues, large number of sources, buffer occupancy distribution, strict hol priority rule, long range dependent sources, two buffers, asymptotic upper bound, scaling parameter, server speed, buffer level, source pooling, numerical validation, reduced service rate approximation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " tail asymptotics, hol priority queues, large number of sources, two buffers, strict priority, buffer tail probability, scaling parameter, server speed, buffer level, source pooling, numerical validation, reduced service rate approximation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: tail asymptotics for hol priority queues handling a large number of independent stationary sources .\nDocument Body: in this paper we study the asymptotics of the tail of the buffer occupancy distribution in buffers accessed by a large number of stationary independent sources and which are served according to a strict hol priority rule . as in the case of single buffers , the results are valid for a very general class of sources which include long range dependent sources with bounded instantaneous rates . we first consider the case of two buffers with one of them having strict priority over the other and we obtain asymptotic upper bound for the buffer tail probability for lower priority buffer . we discuss the conditions to have asymptotic equivalents . the asymptotics are studied in terms of a scaling parameter which reflects the server speed , buffer level and the number of sources in such a way that the ratios remain constant . the results are then generalized to the case of m buffers which leads to the source pooling idea . we conclude with numerical validation of our formulae against simulations which show that the asymptotic bounds are tight . we also show that the commonly suggested reduced service rate approximation can give extremely low estimates .\nKeyphrases (separated by comma):"}
{"0": {"text": " parallel algorithms, very large scale integration, rectangle intersection problem, plane sweep problem, computational geometry, design rule checking, parasitic resistance capacitance extraction, mask processing flows, multicore machines, clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " plane sweep, multicore, VLSI, design rule, mask processing.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " VLSI physical design, computational geometry, design rule checking, RCP extraction, mask processing, embarrassingly parallel, multicore machines, clusters, best known algorithms, real world tests.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, Very Large Scale Integration, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Parasitic Resistance Capacitance Extraction, Mask Processing Flows, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, VLSI Physical Design, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Parasitic Resistance Capacitance Extraction, Mask Processing Flows, Embarrassingly Parallel, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " Parallel Algorithms, VLSI Physical Design, Rectangle Intersection Problem, Plane Sweep Problem, Computational Geometry, Design Rule Checking, Resistance Capacitance Extraction, Mask Processing Flows, Multicore Machines, Clusters.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a variant of parallel plane sweep algorithm for multicore systems .\nDocument Body: parallel algorithms used in very large scale integration physical design bring significant challenges for their efficient and effective design and implementation . the rectangle intersection problem is a subset of the plane sweep problem , a topic of computational geometry and a component in design rule checking , parasitic resistance capacitance extraction , and mask processing flows . a variant of a plane sweep algorithm that is embarrassingly parallel and therefore easily scalable on multicore machines and clusters , while exceeding the best known parallel plane sweep algorithms on real world tests , is presented in this letter .\nKeyphrases (separated by comma):"}
{"0": {"text": " information technology adoption, perceptions, independent variables, antecedents, consequents, expert systems application, technology acceptance models, personal innovativeness, mass media, interpersonal communication", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " user perceptions, information technology, adoption decisions, personal innovativeness, mass media, interpersonal communication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Expert Systems Application, Technology Acceptance Models, Personal Innovativeness, Mass Media, Interpersonal Communication, Perception Development, Adoption Decisions", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nInformation Technology Adoption, Perceptions, Independent Variables, Expert Systems Application, Moderating Influences, Personal Innovativeness, Mass Media, Interpersonal Communication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " information technology adoption, perceptions, independent variables, antecedents, consequents, expert systems application, technology acceptance models, personal innovativeness, mass media, interpersonal communication, perception development, theoretical implications, practical implications.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Expert Systems Application, Technology Acceptance Models, Personal Innovativeness, Mass Media, Interpersonal Communication, Perception Development, Adoption Decisions, Information Technology Adoption", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the antecedents and consequents of user perceptions in information technology adoption .\nDocument Body: a common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables . although a fairly significant body of research that empirically tests these models is now in existence , some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered . this paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application . two research objectives that have both theoretical and practical relevance motivated and guided the study . one , the study challenges an assumption which is implicit in technology acceptance models that of the non existence of moderating influences on the relationship between perceptions and adoption decisions . specifically , the study examines the effects of an important moderating influence personal innovativeness on this relationship . two , the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development . theoretical and practical implications that follow from the results are discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, discrete latent state space, principal component analysis, handwritten digit recognition, satellite image recognition", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variables, generative models, classification accuracy, support vector, perceptron learning, discrete latent, principal component, handwritten digit, satellite image.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, deterministic models, support vector machine, perceptron learning, discrete latent state, principal component analysis, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, discrete latent state space, continuous latent state space, support vector machine, perceptron learning, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, perceptron learning, discrete latent state, continuous latent state, handwritten digit recognition, satellite image recognition, sequential constraint optimization.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " latent variable models, generative models, classification accuracy, support vector machine, discrete latent state, principal component analysis, handwritten digit recognition, satellite image recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: improving classification with latent variable models by sequential constraint optimization .\nDocument Body: in this paper we propose a method to use multiple generative models with latent variables for classification tasks . the standard approach to use generative models for classification is to train a separate model for each class . a novel data point is then classified by the model that attributes the highest probability . the algorithm we propose modifies the parameters of the models to improve the classification accuracy . our approach is made computationally tractable by assuming that each of the models is deterministic , by which we mean that a data point is associated to only a single latent state . the resulting algorithm is a variant of the support vector machine learning algorithm and in a limiting case the method is similar to the standard perceptron learning algorithm . we apply the method to two types of latent variable models . the first has a discrete latent state space and the second , principal component analysis , has a continuous latent state space . we compare the effectiveness of both approaches on a handwritten digit recognition problem and on a satellite image recognition problem .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction Methods, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction, Event Related Potentials, EEG Data, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Feature Extraction, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time, Feature Extraction, Classifiers, Event Related Potentials, EEG Data, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time, Event Related Potentials, Feature Extraction, Classification, Accuracy, Processing Time, EEG Data, Hex O Speller, Prototype System.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " Brain Computer Interface, Real Time Implementation, Feature Extraction Methods, Classifiers, Event Related Potentials, EEG Data Collection, Hex O Speller.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a framework for a real time intelligent and interactive brain computer interface .\nDocument Body: a framework for a real time implementation of a brain computer interface . implementation comparison of different feature extraction methods and classifiers . accuracy processing time comparison for detection of event related potentials erp . an implementation of a prototype system using the proposed bci framework . real time eeg data collection and classification of erps using hex o speller .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, numerical examples, ifr property, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " e m e k, ifr property, interdeparture time, erlang distributions, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interarrival time, service time, erlang distributions, numerical examples, interdeparture time, e m e k [digit] queues, ifr property, theoretical analysis.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nIncreasing Failure Rate, Interdeparture Time, Erlang Distributions, EMEK Queues, Numerical Examples", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, theoretical analysis, numerical results, numerical examples, m > k.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " increasing failure rate, interdeparture time, interarrival time, service time, erlang distributions, e m e k [digit] queues, numerical examples, ifr property, theoretical analysis.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: characterizing output processes of e m e k [digit] queues .\nDocument Body: our goal is to study which conditions of the output process of a queue preserve the increasing failure rate ( ifr ) property in the interdeparture time . we found that the interdeparture time does not always preserve the ifr property , even if the interarrival time and service time are both erlang distributions with ifr . we give a theoretical analysis and present numerical results of e m e k [digit] queues . we show , by numerical examples , that the interdeparture time of e m e k [digit] retains the ifr property if m > k. ( c ) [digit] elsevier ltd. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " low complexity, quadraphonic headsets, surround audio, head related transfer function, simplified reverberator, sound localization, acoustical pole and zero, subjective listening tests, stereo headphones, privacy, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " surround audio, hrtf, capz model, stereo headphones, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " 5.1 channel hearing experience, sound localization capability, acoustical pole and zero model, subjective listening tests, four channel headset, stereo headphones, low cost privacy.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quadraphonic Headsets, Surround Audio, Head Related Transfer Function (HRTF), Moorer's Reverberator, Common Acoustical Pole and Zero (CAPZ) Model", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quadraphonic Headsets, Surround Audio, Head Related Transfer Function, HRTF Model, Simplified Reverberator, Sound Localization, CAPZ Model, Power Consumption, Stereo Headphones, Off-the-Shelf Components.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " four channel headset, 5.1 channel hearing experience, head related transfer function, simplified reverberator, sound localization capability, acoustical pole and zero model, subjective listening tests, stereo headphones, privacy, low cost.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: a low complexity down mixing structure on quadraphonic headsets for surround audio .\nDocument Body: this work presents a four channel headset achieving a 5.1 channel like hearing experience using a low complexity head related transfer function ( hrtf ) model and a simplified reverberator . the proposed down mixing architecture enhances the sound localization capability of a headset using the hrtf and by simulating multiple sound reflections in a room using moorer 's reverberator . since the hrtf has large memory and computation requirements , the common acoustical pole and zero ( capz ) model can be used to reshape the lower order hrtf model . from a power consumption viewpoint , the capz model reduces computation complexity by approximately [digit] % . the subjective listening tests in this study shows that the proposed four channel headset performs much better than stereo headphones . on the other hand , the four channel headset that can be implemented by off the shelf components preserves the privacy with low cost .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, service security, security policy, security personalization, security policy personalization, security personalization approach, personalized security policy.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, security policy, security personalization, service customer, service provider, security preferences.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " malicious individuals, security requirements, security policy, security personalization, flexible security, security policy personalization, security personalization prototype", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nInternet services, E-commerce, E-health, Security policy, Security personalization, Security measures, Service customer, Service provider, Security preferences, Security policy personalization, Security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e-commerce, e-health, service security, security policy, security personalization, security preferences, service providers, service customers, security policy personalization, security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " internet services, e commerce, e health, service security, security policy, security personalization, security preferences, service providers, service customers, personalized security policy, security policy personalization, security personalization prototype.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: security personalization for internet and web services .\nDocument Body: the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy . however , a service customer or consumer may have security preferences that are not reflected in the provider security policy in order for set vice providers to attract and retain customers , as well as reach a wider market , a way of personalizing a security policy to a particular customer is needed we derive the content of an internet or web service security policy and propose a flexible security personalization approach that will allow an internet or web service provider and customer to negotiate to an agreed upon personalized security policy . in addition , we present two application examples of security policy personalization , and overview the design of our security personalization prototype .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, networks on chip, GALS architectures, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network, portable alliance CMOS, standard cell library, [digit] nm CMOS fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " networks on chip, asynchronous converters, synchronous communication, cmos standard, cmos fabrication, spice simulation", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network on chip, various trade offs, latency and robustness, portable alliance cmos, spice simulation, [digit] nm cmos fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nSynchronous Communication, Asynchronous Communication, Networks on Chip, GALS Architectures, CMOS Standard Cell Library", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, networks on chip, GALS architectures, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network, portable alliance CMOS, standard cell library, [digit] nm CMOS fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " synchronous double left right arrow asynchronous converters, high throughput, low latency, globally asynchronous locally synchronous, clusterized multi processor system, fully asynchronous network on chip, latency and robustness, portable alliance cmos, spice simulation, [digit] nm cmos fabrication process.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: two efficient synchronous double left right arrow asynchronous converters well suited for networks on chip in gals architectures .\nDocument Body: this paper presents two high throughput , low latency converters that can be used to convert synchronous communication protocol to asynchronous one and vice versa . we have designed these two hardware components to be used in a globally asynchronous locally synchronous clusterized multi processor system on chip communicating by a fully asynchronous network on chip . the proposed architecture is rather generic , and allows the system designer to make various trade offs between latency and robustness , depending on the selected synchronizer . we have physically implemented the two converters with portable alliance cmos standard cell library and evaluated the architectures by spice simulation for a [digit] nm cmos fabrication process . ( c ) [digit] elsevier b.v. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling and characterization, organization modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, organizational key data, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " process oriented, lead time, degree of autonomy, improvement potentialities, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organization modeling system, structuring strategies, dynamical analysis, evaluation module, logistical key data, organizational key data, improvement potentialities, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " Simulation Aided Design, Manufacturing Systems, Structuring Strategies, Forecasted Order Program, Activity Networks, Organization Modeling System, Process Oriented Strategy, Dynamical Analysis, Simulation Tool Femos, Logistical Key Data, Organizational Key Data, Gear Boxes, Robot Grip Arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling and characterization, organization modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, degree of autonomy, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " simulation aided design, organizational structures, manufacturing systems, structuring strategies, modeling system, process oriented strategy, dynamical analysis, evaluation module, logistical key data, organizational key data, case study, gear boxes, robot grip arms.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: simulation aided design of organizational structures in manufacturing systems using structuring strategies .\nDocument Body: this paper presents a simulation aided approach for designing organizational structures in manufacturing systems . the approach is based on a detailed modeling and characterization of the forecasted order program , especially of elementary processes , activity networks and manufacturing orders . under the use of the organization modeling system form , that has been developed at the ifab institute of human and industrial engineering of the university of karlsruhe , structuring strategies e.g. , a process oriented strategy can be applied in order to design organizational structures in manufacturing systems in a flexible and efficient way . following that , a dynamical analysis of the created manufacturing structures can be carried out with the simulation tool femos , that has also been developed at the ifab institute . the evaluation module of femos enables to measure the designed solutions with the help of logistical e.g. , lead time degree and organizational e.g. , degree of autonomy key data . this evaluation is the basis for the identification of effective manufacturing systems and also of improvement potentialities . finally , a case study is presented in this paper designing and analyzing different organizational structures of a manufacturing system where gear boxes and robot grip arms were manufactured .\nKeyphrases (separated by comma):"}
{"0": {"text": " selectors, combinatorial structures, pseudo random generators, broadcasting protocols, wake up problem, coin weighting problems, non constructive bounds, common technique.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " selectors, combinatorial objects, pseudo random, broadcasting, wake up, test sets, coin weighting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " explicit constructions, combinatorial objects, pseudo random generators, deterministic protocols, broadcasting, unknown directed radio networks, wake up problem, coin weighting problems, non constructive bounds, common technique.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nSelectors, Combinatorial Structures, Pseudo Random Generators, Broadcasting Protocols, Coin Weighting Problems", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " explicit constructions, combinatorial objects, selectors, selective families, pseudo random generators, broadcasting protocols, wake up problem, coin weighting problems, deterministic protocols, unknown directed radio networks.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " broadcasting protocols, coin weighting problems, directed radio networks, deterministic protocols, pseudo random generators, selective families, test sets, wake up problem", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: explicit constructions of selectors and related combinatorial structures , with applications .\nDocument Body: in this paper we present explicit constructions of several combinatorial objects selectors cgr00 and selective families cggpr00 , pseudo random generators for proof systems abrw00 and fixed waking schedules gpp00 . as a result , we obtain almost optimal deterministic protocols for broadcasting in unknown directed radio networks cgr00 and wake up problem gpp00 . we also show application of selectors ( and its variants ) to explicit construction of test sets for coin weighting problems dh00 . the parameters of our constructions come close to the best known non constructive bounds . the constructions are achieved using a common technique , which could be of use for other problems .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, membrane analogy, maximum shearing stress, prismatic torsional members, fortran code, delphi based environment, multiple level application, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " h refinement, membrane analogy, maximum shearing stress, torsional analysis, l section.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " membrane analogy, h refinement type, a posteriori error estimates, maximum shearing stress, prismatic torsional members, integrated fortran code, delphi based environment, multiple level application, analytic solutions, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nMembrane Analogy, Finite Element Refinement, Torsional Problems, Maximum Shearing Stress, A Posteriori Error Estimates", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, a posteriori error estimates, graphical procedure, maximum shearing stress, prismatic torsional members, membrane analogy, multiple level application, analytic solutions, l section member, very accurate calculation.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " selective finite element refinement, h refinement type, a posteriori error estimates, maximum shearing stress, membrane analogy, fortran code, delphi based environment, multiple level application, cross sectional geometries, analytic solutions, l section member.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: selective finite element refinement in torsional problems based on the membrane analogy .\nDocument Body: this work presents a selective finite element refinement strategy based on the h refinement type , in the context of a posteriori error estimates considerations ( error computed after the application of the proposed refining scheme ) , based on a graphical procedure to determine progressively better estimates for the maximum shearing stress in prismatic torsional members . it is structured in an integrated fortran code and delphi based environment to refine an initial arbitrary finite element mesh . the proposed procedure is founded on the membrane analogy that exists between membrane deflections and the torsion problem in the sense that the location of the membrane largest gradient drives the refining procedure . it is shown that multiple level application of the proposed method to two members with different cross sectional geometries with known analytic solutions leads to progressively more accurate estimates ( < 1.0 % error in most cases ) for the maximum shearing stresses calculations . finally , the proposed method is applied to the torsional analysis of an l section member , showing that for this practical case the procedure results in a very accurate calculation as well .\nKeyphrases (separated by comma):"}
{"0": {"text": " RNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Parallel Processing, Duplicate Processing, Modular Multiplication", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " rns montgomery, modular exponentiation, base transformations, duplicate processing, parallel processing, modular multiplication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " modular exponentiation, residue number systems, parallel processing, base transformations, montgomery multiplication, speedup, duplicate processing, upper limit, modular multiplication.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nRNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Parallel Processing", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " RNS Montgomery Multiplication, Modular Exponentiation, Residue Number Systems, Base Transformations, Duplicate Processing, Parallel Processing, Speedup, Modular Multiplication, Upper Limit, Number of Parallel Processing Units.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " modular exponentiation, residue number systems, rns montgomery multiplication, base transformations, parallel processing, speedup, modular multiplication, upper limit.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: rns montgomery multiplication algorithm for duplicate processing of base transformations .\nDocument Body: this paper proposes a new algorithm to achieve about two times speedup of modular exponentiation which is implemented by montgomery multiplication based on residue number systems ( rns ) . in rns montgomery multiplication , its performance is determined by two base transformations dominantly . for the purpose of realizing parallel processing of these base transformations , i.e. duplicate processing , we present two procedures of rns montgomery multiplication , in which rns bases a and b are interchanged , and perform them alternately in modular exponentiation iteration . in an investigation of implementation , 1.87 times speedup has been obtained for [digit] bit modular multiplication . the proposed rns montgomery multiplication algorithm has an advantage in achieving the performance corresponding to that the upper limit of the number of parallel processing units is doubled .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, Grounded Design, Usability Problem, Value, HCI, Contextual Description, Prescriptive Approaches", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " quality in use, grounded design, usability problem, value, hci.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " grounded design, usability problem, contextual description, prescriptive approaches, value in the world, potential of HCI", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, HCI, Grounded Design, Usability Problem, Value in the World.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, HCI, Grounded Design, Usability Problem, Contextual Description, Value in the World, Fit between Designs, Expected Contexts of Use, Prescriptive Approaches, Dilemmas about Value.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " Quality in Use, Grounded Design, Usability Problem, Value Delivery, Contextual Description, Prescriptive Approaches.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: from quality in use to value in the world .\nDocument Body: this paper argues that a focus on quality in use limits the potential of hci . it summarizes how novel approaches such as grounded design can let us go beyond usability to reveal the fit between designs and expected contexts of use . this however is still not enough . it can not resolve dilemmas about what is and is not a usability problem , or when fit is or is not essential . such dilemmas can only be resolved by an understanding of the value that artifacts aim to deliver in the world . hci must move beyond contextual description to prescriptive approaches to value in the world .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, vertebrate family [digit] GPCR members, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit], gpcrs, fugu rubripes, vertebrates, phylogenetic analysis, calr cgrpr, crf.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Japanese pufferfish, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu Rubripes, vertebrates, phylogenetic analysis, Calr CGRPR, CRF.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu Rubripes, vertebrates, phylogenetic analysis, Calr Cgrpr, CRF, receptor group, common ancestral gene, Japanese Pufferfish, comparative study.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " family [digit] GPCRs, Fugu rubripes, phylogenetic analysis, Calr CGRPR, CRF, vertebrate family [digit] GPCR members, common ancestral gene precursor.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: comparative study of family [digit] gpcrs in fugu rubripes .\nDocument Body: abstract in this study , members of family [digit] gpcrs , one of the largest families of receptors in vertebrates , were isolated and characterized in the genome of the japanese pufferfish , fugu rubripes , and compared with the orthologous genes in other vertebrates . phylogenetic analysis carried out with all vertebrate family [digit] gpcr members indicated that calr cgrpr and crf are the most divergent receptor group within this family and that the remaining members appear to originate from a common ancestral gene precursor .\nKeyphrases (separated by comma):"}
{"0": {"text": " blotto game, low complexity, fair multiuser, subcarrier allocation, uplink OFDMA networks, limited budget, optimal throughput, competitive fairness, Nash equilibrium, low complexity algorithm, uncorrelated fading channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " blotto game, ofdma networks, limited budget, nash equilibrium, low complexity", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA, Power Allocation, Budget Allocation, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading, Correlated Fading.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA Networks", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Low Complexity, Fair Multiuser, Subcarrier Allocation, Uplink OFDMA Networks, Limited Budget, Optimal Throughput, Competitive Fairness, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading Channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " Blotto Game, Subcarrier Allocation, OFDMA Networks, Limited Budget, Power Allocation, Budget Allocation, Competitive Fairness, Nash Equilibrium, Low Complexity Algorithm, Uncorrelated Fading Channels.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: blotto game based low complexity fair multiuser subcarrier allocation for uplink ofdma networks .\nDocument Body: this article presents a subcarrier allocation scheme based on a blotto game ( sabg ) for orthogonal frequency division multiple access ( ofdma ) networks where correlation between adjacent subcarriers is considered . in the proposed game , users simultaneously compete for subcarriers using a limited budget . in order to win as many good subcarriers as possible in this game , users are required to wisely allocate their budget . efficient power and budget allocation strategies are derived for users for obtaining optimal throughput . by manipulating the total budget available for each user , competitive fairness can be enforced for the sabg . in addition , the conditions to ensure the existence and uniqueness of nash equilibrium ( ne ) for the sabg are also established . an low complexity algorithm that ensures convergence to ne is proposed . simulation results show that the proposed low complexity sabg can allocate resources fairly and efficiently for both uncorrelated and correlated fading channels .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, glass transition temperature, relative dielectric constant, loss factors, curing agent content, mekp concentrations, styrene content, average molecular mass, vickers hardness, tape casting, relative dielectric constant.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, mekp amount, styrene content, relative dielectric constant, loss factor, molecular mass, viscosity, tape casting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, glass transition temperature, relative dielectric constant, loss factors, curing agent content, mekp concentrations, styrene content, average molecular mass, vickers hardness, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, curing agent, styrene content, glass transition temperature, relative dielectric constant, loss factors, molecular mass, vickers hardness, mekp concentrations, film formation, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, unsaturated polyesterstyrene copolymers, glass transition temperature, relative dielectric constant, loss factors, curing agent, mekp amount, styrene content, average molecular mass, vickers hardness, tape casting.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " polymerization conditions, curing agent, styrene content, glass transition temperature, relative dielectric constant, loss factors, molecular mass, vickers hardness, mekp concentrations, film formation, tape casting, viscosity, ambient conditions.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: polymerization conditions influence on the thermomechanical and dielectric properties of unsaturated polyesterstyrene copolymers .\nDocument Body: the influence of different polymerization conditions like curing agent ( mekp ) amount and styrene content on the glass transition temperature , the relative dielectric constant as well as loss factors of unsaturated polyesterstyrene polymer systems after solidification was investigated in depth . with respect to a high average molecular mass and vickers hardness a curing agent content of 3wt % is recommendable . increasing mekp concentrations cause a slight elevation of the polymers relative dielectric constant as well as of the loss factor . regarding an easy film formation using tape casting a.o. higher styrene amounts lower the viscosity of the resin significantly , the relative dielectric constant and the loss factor decrease also . as an average value a relative dielectric constant of [digit] under ambient conditions can be obtained .\nKeyphrases (separated by comma):"}
{"0": {"text": " online information, pseudo online information, stroke order independent information, classification decisions, cursive word recognition, single engine, pure online information", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " online data, pseudo online, cursive word, single engine.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, single engine, classification decisions, online cursive words, offline representation, improved recognition", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": "\nOnline Information, Pseudo Online Information, Stroke Order Independent Information, Cursive Word Recognition, Classification Decisions, Single Engine, Pure Online Information.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, online cursive words, single engine, classification decisions, offline representation, online signal, combination of classifiers, novel method, improved recognition.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " stroke order independent information, pseudo online representation, online cursive words, single engine, classification decisions, offline representation, online signal, word recognition, combination of classifiers.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: an integration of online and pseudo online information for cursive word recognition .\nDocument Body: in this paper , we present a novel method to extract stroke order independent information from online data . this information , which we term pseudo online , conveys relevant information on the offline representation of the word . based on this information , a combination of classification decisions from online and pseudo online cursive word recognizers is performed to improve the recognition of online cursive words . one of the most valuable aspects of this approach with respect to similar methods that combine online and offline classifiers for word recognition is that the pseudo online representation is similar to the online signal and , hence , word recognition is based on a single engine . results demonstrate that the pseudo online representation is useful as the combination of classifiers perform better than those based solely on pure online information .\nKeyphrases (separated by comma):"}
{"0": {"text": " gaussian pulses, mobile telephony, gmsk signals, uwb communications, third order derivatives, rectangular pulses, spectral mask, relative error, energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " gaussian pulses, gsm, uwb, derivatives, rectangular pulses, filter.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " mobile telephony (GSM), UWB communications, second order derivatives, third order derivatives, spectral mask requirements, relative error, energy spectra", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, Communications, Mobile telephony, GMSK signals, UWB communications, Derivatives, Rectangular pulses, Filter, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, GSM signals, UWB communications, Physical implementations, Third order derivatives, Rectangular pulses, Fixed amplitudes, Standard techniques, Spectral mask requirements, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " Gaussian pulses, Mobile telephony, GMSK signals, UWB communications, Third order derivatives, Rectangular pulses, Spectral mask, Relative error, Energy spectra.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: generation of quasi gaussian pulses based on correlation techniques .\nDocument Body: the gaussian pulses have been mostly used within communications , where some applications can be emphasized mobile telephony ( gsm ) , where gmsk signals are used , as well as the uwb communications , where short period pulses based on gaussian waveform are generated . since the gaussian function signifies a theoretical concept , which can not be accomplished from the physical point of view , this should be expressed by using various functions , able to determine physical implementations . new techniques of generating the gaussian pulse responses of good precision are approached , proposed and researched in this paper . the second and third order derivatives with regard to the gaussian pulse response are accurately generated . the third order derivates is composed of four individual rectangular pulses of fixed amplitudes , being easily to be generated by standard techniques . in order to generate pulses able to satisfy the spectral mask requirements , an adequate filter is necessary to be applied . this paper emphasizes a comparative analysis based on the relative error and the energy spectra of the proposed pulses .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, convex semi definite programming, support vector machines, expected risk, margin, support vectors, data compression, generalization bound, prediction ability, robust form, soft idea, numerical results, generalization ability, regular pca.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, svpca, svms, convexity, semi definite", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, support vector machines, expected risk, margin, semi definite programming, data compression, generalization bound, prediction ability, robust form, convexity, numerical results, generalization ability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Linear PCA, Support Vector Machines, Expected Risk, Margin, Semi Definite Programming, Weighted Case, Generalization Bound, Soft Idea, Convexity, Local Minima, Numerical Results, Generalization Ability.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " Linear PCA, Convex Semi Definite Programming, Support Vector Machines, Expected Risk, Margin, Support Vectors, Data Compression, Generalization Bound, Soft Idea, Robust Form, Local Minima, Numerical Results.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " linear pca, convex semi definite programming, support vector machines, expected risk, margin, support vectors, data compression, generalization bound, prediction ability, robust form, soft idea, numerical results, generalization ability, local minima.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: learning linear pca with convex semi definite programming .\nDocument Body: the aim of this paper is to learn a linear principal component using the nature of support vector machines ( svms ) . to this end , a complete svm like framework of linear pca ( svpca ) for deciding the projection direction is constructed , where new expected risk and margin are introduced . within this framework , a new semi definite programming problem for maximizing the margin is formulated and a new definition of support vectors is established . as a weighted case of regular pca , our svpca coincides with the regular pca if all the samples play the same part in data compression . theoretical explanation indicates that svpca is based on a margin based generalization bound and thus good prediction ability is ensured . furthermore , the robust form of svpca with a interpretable parameter is achieved using the soft idea in svms . the great advantage lies in the fact that svpca is a learning algorithm without local minima because of the convexity of the semi definite optimization problems . to validate the performance of svpca , several experiments are conducted and numerical results have demonstrated that their generalization ability is better than that of regular pca . finally , some existing problems are also discussed .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, NAT, Metathesaurus, Semantic Network, Semantic Type, Diagrammatic Displays, Hybrid Diagram Text Interface, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " umls, nat, auditing, hybrid interface, semantic type, metathesaurus, concept networks.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases with less than 3 words. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS auditing, Neighborhood Auditing Tool, Hybrid Diagram Text Interface, Metathesaurus Level, Semantic Network Level, Case Studies, Select Group of Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are not present in the original corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Source Vocabularies, Metathesaurus, Semantic Network, Diagrammatic Displays, Text Windows, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases in 5 different categories. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Source Vocabularies, Inconsistencies, Focus Concept, Concept Networks, Diagrammatic Displays, Metathesaurus, Semantic Network, Semantic Type, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate at least 10 keyphrases that are diverse to each other. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}
{"0": {"text": " UMLS, Neighborhood Auditing Tool, Hybrid Interface, Semantic Network, Metathesaurus, Concept Network, Diagrammatic Displays, Text Windows, Case Studies, Auditors.", "index": 0, "logprobs": null, "finish_reason": "stop"}, "prompt": "Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases that are diverse to each other and not present in the corpus. You should separate the keyphrases with a comma.\n\nDocument Title: the neighborhood auditing tool a hybrid interface for auditing the umls .\nDocument Body: the umls 's integration of more than [digit] source vocabularies , not necessarily consistent with one another , causes some inconsistencies . the purpose of auditing the umls is to detect such inconsistencies and to suggest how to resolve them while observing the requirement of fully representing the content of each source in the umls . a software tool , called the neighborhood auditing tool ( nat ) , that facilitates umls auditing is presented . the nat supports neighborhood based auditing , where , at any given time , an auditor concentrates on a single focus concept and one of a variety of neighborhoods of its closely related concepts . typical diagrammatic displays of concept networks have a number of shortcomings , so the nat utilizes a hybrid diagram text interface that features stylized neighborhood views which retain some of the best features of both the diagrammatic layouts and text windows while avoiding the shortcomings . the nat allows an auditor to display knowledge from both the metathesaurus ( concept ) level and the semantic network ( semantic type ) level . various additional features of the nat that support the auditing process are described . the usefulness of the nat is demonstrated through a group of case studies . its impact is tested with a study involving a select group of auditors . ( c ) [digit] elsevier inc. all rights reserved .\nKeyphrases (separated by comma):"}